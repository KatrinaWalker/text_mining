{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining | Problem Set 1\n",
    "### David Rosenfeld & Katrina Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Pre-Processing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_origin = pd.read_table(\"speech_data_extend.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract only the second column of the table\n",
    "data = data_origin.iloc[:,1]\n",
    "\n",
    "# Convert each document into lowercase\n",
    "data = [element.lower() for element in data] \n",
    "\n",
    "# Tokenize data\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = [word_tokenize(element) for element in data]\n",
    "\n",
    "# Remove non-alphabetic characters\n",
    "data = [[ w for w in doc if (w != '' and w not in string.punctuation and not any(char.isdigit() for char in w)) ] for doc in data]\n",
    "\n",
    "# Remove stopwords using a list of your choice\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data = [[ w for w in doc if w  not in stop_words ] for doc in data]\n",
    "\n",
    "# Stem the data using the Porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "data = [[ ps.stem(w) for w in doc] for doc in data]\n",
    "\n",
    "# Compute the corpus-level tf-idf score\n",
    "all_words = sum(data, [])\n",
    "df = {key: 0 for key in all_words}\n",
    "\n",
    "# Iterator: for each word, iterate over every document. If word is present in text, +1 to value for that word.\n",
    "\n",
    "for key,value in df.items():\n",
    "    for doc in data:\n",
    "        if key in doc:\n",
    "            df[key] +=1 # add one for each document for which word appears\n",
    "\n",
    "\n",
    "# Initialise a dictionary to compute the idf for each term\n",
    "idf = {key: 0 for key in all_words}\n",
    "ndocs = len(data)\n",
    "\n",
    "# Compute idf for each word\n",
    "for key,value in df.items():\n",
    "    idf[key] = np.log(ndocs/df[key])\n",
    "\n",
    "# Compute term frequency for each term & initialise dictionary\n",
    "tf = {key: 0 for key in all_words}\n",
    "\n",
    "# Calculate corpus-wide tf-idf for each term\n",
    "tf_idf = {key: 0 for key in all_words}\n",
    "for key,value in tf_idf.items():\n",
    "    tf_idf[key] = tf[key]*idf[key]\n",
    "    \n",
    "# Plot corpus-wide tf_idf\n",
    "y = sorted(tf_idf.values(), reverse = True)\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "# Select only the terms with frequency higher than 20. Create a new dictionary and plot it.\n",
    "d = {k: v for k, v in tf_idf.items() if v > 25}\n",
    "\n",
    "y = sorted(d.values(), reverse = True)\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "# Save new words under new_words\n",
    "new_words = [key for key in d]\n",
    "\n",
    "# Construct a document-term matrix\n",
    "dt_matrix = pd.DataFrame([[doc.count(w) for doc in data] for w in new_words])\n",
    "dt_matrix.index = new_words\n",
    "dt_matrix.shape\n",
    "\n",
    "# Save the document-term matrix for future use\n",
    "dt_matrix.to_csv(\"dt_matrix.csv\", sep = \";\", index=True)\n",
    "\n",
    "# Retrieve the saved document-term matrix\n",
    "dt_matrix = pd.DataFrame.from_csv(\"dt_matrix.csv\", sep = \";\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Perform Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A create dictionary to assess heterogeneaity\n",
    "d = pd.read_excel(\"LoughranMcDonald_MasterDictionary_2014.xlsx\")\n",
    "dic = d[['Word', 'Negative']].copy()\n",
    "dic = dic[dic.Negative != 0]\n",
    "dic = dic[['Word']].copy()\n",
    "dic_stemmer = PorterStemmer()\n",
    "\n",
    "dic_list = dic['Word'].tolist()\n",
    "dic_s = [str(w).lower() for w in dic_list]\n",
    "dic = [dic_stemmer.stem(x) for x in dic_s]\n",
    "dic = set(dic)\n",
    "dic = [x for x in iter(dic)]\n",
    "\n",
    "data_2 = [' '.join(doc) for doc in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=dic)\n",
    "cv_results = cv.fit_transform(data_2)\n",
    "cv_results_test = cv_results.todense()\n",
    "cv_freq = np.sum(cv_results_test, axis = 1)\n",
    "cv_freq_norm = cv_freq\n",
    "for i in np.arange(len(data)):\n",
    "    cv_freq_norm[i] = 100*(cv_freq_norm[i]/len(data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with years where most months were in recession as 1 and 0 otherwise\n",
    "recession = pd.read_csv(\"/Users/davidrosenfeld/Documents/text_mining_course/text_mining/USREC.csv\", index_col = False)\n",
    "recession['DATE'] = pd.to_datetime(recession['DATE'], format='%Y-%m-%d')\n",
    "recession = recession.groupby(recession['DATE'].map(lambda x: x.year)).mean().round()\n",
    "recession['year'] = recession.index\n",
    "\n",
    "# Select data only for years above 1854 and join the recession indicator for the relevant years\n",
    "data_rec = data_origin[data_origin['year'] >= 1854]\n",
    "data_rec = data_rec.join(recession['USREC'], on = 'year', how = 'left')\n",
    "\n",
    "# Select the term frequencies for the dictionary for the documents from 1854\n",
    "cv_corr = pd.DataFrame(cv_freq_norm)\n",
    "cv_corr.index = data_origin['year']\n",
    "cv_corr = cv_corr[cv_corr.index >= 1854]\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "corr_rec = np.corrcoef(cv_corr.iloc[:,0], data_rec['USREC'])\n",
    "print(\"the correlation coefficient between our dictionary and the data is\", corr_rec[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dic = {key: 0 for key in dic}\n",
    "for key,value in df_dic.items():\n",
    "    for doc in data:\n",
    "        if key in doc:\n",
    "            df_dic[key] +=1 # add one for each document for which word appears\n",
    "df_dic = {key: value for key,value in df_dic.items() if df_dic[key] !=0}\n",
    "idf_dic = {key: 0 for key in df_dic if df_dic[key] !=0}\n",
    "ndocs = len(data)\n",
    "# Compute idf for each word\n",
    "for key,value in df_dic.items():\n",
    "    idf_dic[key] = np.log(ndocs/df_dic[key])\n",
    "    \n",
    "    \n",
    "new_dic = [key for key in df_dic]\n",
    "               \n",
    "dt_dic = pd.DataFrame([doc.count(w) for doc in data] for w in new_dic)\n",
    "dt_dic.index = new_dic\n",
    "tf_dic = pd.DataFrame(np.where(dt_dic == 0, 0, 1 + np.log(dt_dic)))\n",
    "tf_dic.index = new_dic\n",
    "tf_idf_dic = tf_dic\n",
    "for w in tf_idf_dic.index:\n",
    "    tf_idf_dic.loc[w] = tf_dic.loc[w]*idf_dic[w]\n",
    "tf_idf_dic = np.transpose(tf_idf_dic)\n",
    "tf_idf_avg = np.sum(tf_idf_dic, axis = 1)\n",
    "cv_corr2 = pd.DataFrame(tf_idf_avg)\n",
    "cv_corr2.index = data_origin['year']\n",
    "cv_corr2 = cv_corr2[cv_corr2.index >= 1854]\n",
    "corr2_rec = np.corrcoef(cv_corr2.iloc[:,0], data_rec['USREC'])\n",
    "print(\"the correlation coefficient between our dictionary and the data is\", corr2_rec[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Perform Singular Value Decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a term-frequency matrix\n",
    "tf = pd.DataFrame(np.where(dt_matrix == 0, 0, 1 + np.log(dt_matrix)))\n",
    "tf.index = dt_matrix.index\n",
    "\n",
    "# Create a tf_idf matrix\n",
    "tf_idf = tf\n",
    "#tf_idf.index = tf.index\n",
    "\n",
    "for w in tf_idf.index:\n",
    "    tf_idf.loc[w] = tf.loc[w]* idf[w]\n",
    "\n",
    "# Save the tf_idf matrix to a csv file for future use\n",
    "tf_idf.to_csv(\"tf_idf.csv\", sep = \";\", index=True)\n",
    "\n",
    "tf_idf = pd.DataFrame.from_csv(\"tf_idf.csv\", sep = \";\", index_col = 0)\n",
    "tf_idf = np.transpose(tf_idf)\n",
    "\n",
    "\n",
    "# Compute an SVD of the tf_idf matrix\n",
    "U, s, V = np.linalg.svd(tf_idf)\n",
    "U.shape, s.shape, V.shape\n",
    "\n",
    "# Calculate and plot the proportion of variance explained by the singular values\n",
    "pve = s/sum(s)\n",
    "plt.plot(pve)\n",
    "plt.show()\n",
    "\n",
    "# Construct an approximate tf_idf matrix with the first 200 principal components\n",
    "U_hat = U[:,0:200]\n",
    "s_hat = s[0:200]\n",
    "V_hat = V[0:200, 0:200]\n",
    "tf_idf_hat = np.matmul(np.matmul(U_hat, np.diag(s_hat)), V_hat)\n",
    "tf_idf_hat.shape\n",
    "\n",
    "# Create a dataframe with years where most months were in recession as 1 and 0 otherwise\n",
    "recession = pd.read_csv(\"USREC.csv\", index_col = False)\n",
    "recession['DATE'] = pd.to_datetime(recession['DATE'], format='%Y-%m-%d')\n",
    "recession = recession.groupby(recession['DATE'].map(lambda x: x.year)).mean().round()\n",
    "recession['year'] = recession.index\n",
    "\n",
    "# Create index for recession years and for growth years\n",
    "rec = [i for i in range(len(recession.index)) if recession.iloc[i,0] == 1]\n",
    "grow = [i for i in range(len(recession.index)) if recession.iloc[i,0] == 0]\n",
    "\n",
    "# Compute cosine similarities between all documents, and subset to get those within recession years, \n",
    "# within growth years, and between recession and growth years\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos = cosine_similarity(tf_idf)\n",
    "cosine_rec = cos[rec,:][:,rec]\n",
    "cosine_grow = cos[grow,:][:, grow]\n",
    "cosine_cross = cos[rec,:][:, grow]\n",
    "\n",
    "# Print result\n",
    "print('cosine similarity within recession years', np.mean(cosine_rec))\n",
    "print('cosine similarity within growth years', np.mean(cosine_grow))\n",
    "print('cosine similarity between recession and growth years', np.mean(cosine_cross))\n",
    "\n",
    "# Start again with tf_idf_hat\n",
    "cos_hat = cosine_similarity(tf_idf_hat)\n",
    "cosine_rec_hat = cos_hat[rec,:][:,rec]\n",
    "cosine_grow_hat = cos[grow,:][:, grow]\n",
    "cosine_cross_hat = cos[rec,:][:, grow]\n",
    "\n",
    "# Print result\n",
    "print('cosine similarity within recession years', np.mean(cosine_rec_hat))\n",
    "print('cosine similarity within growth years', np.mean(cosine_grow_hat))\n",
    "print('cosine similarity between recession and growth years', np.mean(cosine_cross_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
