{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "#### Implement the uncollapsed Gibbs sampler for latent Dirichlet allocation we discussed in class. Apply it to state-of-the-union addresses at a level of aggregation you choose and describe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lda\n",
    "from collections import Counter\n",
    "import scipy.sparse as ssp\n",
    "import topicmodels as tpm\n",
    "from nltk import word_tokenize\n",
    "from numpy.random import dirichlet\n",
    "import time\n",
    "\n",
    "\n",
    "###############\n",
    "# Data Prep\n",
    "###############\n",
    "\n",
    "data = pd.read_table(\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data = data.loc[data['year']>=1946]\n",
    "data = data.reset_index()\n",
    "\n",
    "def data_preparation(data):\n",
    "    prep_data = data.apply(lambda row: # tokenize\n",
    "                            word_tokenize(row['speech'].lower()), axis=1)\n",
    "    stop_w=set(stopwords.words('english')) # stopwords\n",
    "    for i in range(len(prep_data)): # non-alphanum characters\n",
    "        prep_data[i] = [w for w in prep_data[i] if w not in stop_w and w.isalpha()]\n",
    "    stemmer = PorterStemmer() # create stemmer object\n",
    "    for i in range(len(prep_data)): # stem data\n",
    "        prep_data[i] = [stemmer.stem(elem) for elem in prep_data[i]]\n",
    "    unique_words = np.unique([word for doc in prep_data for word in doc]).tolist()\n",
    "    return prep_data, unique_words\n",
    "\n",
    "\n",
    "prep_data, unique_words = data_preparation(data)\n",
    "\n",
    "\n",
    "###############\n",
    "# Uncollapsed Gibbs\n",
    "###############\n",
    "\n",
    "### Aux functions Z sample\n",
    "def simulate(K, row):\n",
    "    samples = np.random.multinomial(1,[1/K]*K,len(prep_data[row])).tolist()\n",
    "    samples_correct = []\n",
    "    for s in samples:\n",
    "        samples_correct.append(s.index(1))\n",
    "    return samples_correct\n",
    "\n",
    "def N_count(Z_d, K):\n",
    "    N_count_vector = []\n",
    "    for k in range(K):\n",
    "        N_count_vector.append(Z_d.count(k))\n",
    "    return N_count_vector\n",
    "\n",
    "### Sample for topic allocation\n",
    "def sample_topic(Z, theta, beta):\n",
    "    D = len(Z)\n",
    "    for d in range(D):\n",
    "        n = len(Z[d])\n",
    "        for i in range(n):\n",
    "            beta_v = beta[unique_words.index(prep_data[d][i])]\n",
    "            probs = (theta[d,:]*beta_v)/np.sum(theta[d,:]*beta_v)\n",
    "            Z[d][i] = np.random.multinomial(1, probs).tolist().index(1)\n",
    "    return Z #sample_topic(Z, theta, beta)\n",
    "\n",
    "### Sample for theta\n",
    "def sample_theta(Z,alpha,theta):\n",
    "    D,K = theta.shape\n",
    "    N = np.zeros((D,K))\n",
    "    for d in range(D):\n",
    "        #N[d,:] = np.unique(Z[d], return_counts=True)[1]\n",
    "        N[d,:] = N_count(Z[d], K)\n",
    "        theta[d,:] = dirichlet(N[d,:] + alpha)\n",
    "    return theta\n",
    "\n",
    "### Sample for beta\n",
    "def sample_beta(Z,prep_data,eta,beta):\n",
    "    K = beta.shape[1]\n",
    "    M = np.zeros((K,V))\n",
    "    # Generate M\n",
    "    s = [i for sublist in prep_data for i in sublist ]\n",
    "    z_s = [z for sublist in Z for z in sublist]\n",
    "    for k in range(K):\n",
    "        words = [s[i] for i in range(len(s)) if z_s[i] == k]\n",
    "        counts = Counter(words)\n",
    "        for v in range(len(unique_words)):\n",
    "            if unique_words[v] in counts: M[k,v] = counts[unique_words[v]]\n",
    "    # Generate beta\n",
    "    for k in range(K):\n",
    "        beta[:,k] = dirichlet(M[k,:] + eta)\n",
    "    return beta\n",
    "\n",
    "# beta = sample_beta(Z, prep_data, eta,beta)\n",
    "def gibbs_sampler(n_iter,prep_data,alpha,eta,K):\n",
    "    ## Initialize objects\n",
    "    D = len(prep_data)\n",
    "    theta = dirichlet([alpha]*K,D)\n",
    "    beta = dirichlet([eta]*K,V)\n",
    "    Z = prep_data.apply(lambda row: simulate(K,row))\n",
    "    Z_dist = []\n",
    "    theta_dist = []\n",
    "    beta_dist = []\n",
    "    for i in range(n_iter):\n",
    "        print('Iteration nº:'+ str(i))\n",
    "        start = time.time()\n",
    "        Z = sample_topic(Z,theta,beta)\n",
    "        theta = sample_theta(Z,alpha,theta)\n",
    "        beta = sample_beta(Z,prep_data,eta,beta)\n",
    "        Z_dist.append(Z)\n",
    "        theta_dist.append(theta)\n",
    "        beta_dist.append(beta)\n",
    "        print('Duration:'+ str(time.time()-start))\n",
    "    return Z_dist, theta_dist, beta_dist\n",
    "\n",
    "### Initial parameters\n",
    "# D = len(prep_data) # of documents\n",
    "# V = len(unique_words) # of unique terms\n",
    "# Z = prep_data.apply(lambda row: simulate(K,row)) # Z_dn\n",
    "# N = np.zeros((D,K))\n",
    "# M = np.zeros((K,V))\n",
    "\n",
    "# Values\n",
    "K = 10 \n",
    "alpha = 50/K\n",
    "eta = 200/V\n",
    "\n",
    "Z_2, theta_2, beta_2 = gibbs_sampler(5000, prep_data, alpha, eta, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "#### Find and run a version of the collapsed Gibbs sampler using lda in python. Run it for the same parameter values (i.e. Dirichlet hyperparameters and K) and documents you chose for the question above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urrllib2\n",
    "\n",
    "# Import the original data\n",
    "data_origin = pd.read_table(\"speech_data_extend.txt\", encoding=\"utf-8\") \n",
    "url = urllib2.urlopen('https://www.dropbox.com/s/fcjeg9uxmsu2gn2/dt_matrix.csv?dl=0')\n",
    "\n",
    "dt_matrix = pd.DataFrame.from_csv(\"dt_matrix.csv\", sep = \";\", index_col = 0) \n",
    "\n",
    "X = dt_matrix.iloc[:][data_origin.year >= 1945]\n",
    "X = ssp.csr_matrix(X.astype(int))\n",
    "\n",
    "K = 2\n",
    "\n",
    "Col_Gibbs = tpm.LDA.LDAGibbs(prep_data,K)\n",
    "\n",
    "Col_Gibbs.alpha\n",
    "Col_Gibbs.beta\n",
    "\n",
    "burn_samples = 1000\n",
    "jumps = 50\n",
    "used_samples = 10\n",
    "\n",
    "Col_Gibbs.sample(burn_samples,jumps,used_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Plot the perplexity across sampling iterations beginning from several diﬀerent starting values for each algorithm. Which algorithm appears to burn in faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Col_Gibbs.perplexity() \n",
    "\n",
    "word_topics = Col_Gibbs.tt \n",
    "doc_topics = Col_Gibbs.dt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) For the best-ﬁt chain from each algorithm after the burn-in period, construct estimates of the predictive distribution of θd for each document across a number of draws. Recall that the kth element of the predictive distribution is the number of times topic k is observed in document d plus α over Kα + Nd. Are the average values of these predictive distributions similar in the uncollapsed and collapsed samplers? How variable are these predictive distributions in the two algorithms across sample draws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "#### Compare the classiﬁcation performance of a penalized logistic regression (which is implemented in scikit learn) when paragraphs are represented as unigram counts over raw terms versus topic shares. Use training samples to estimate the relationship between document content and political party, and then assess its out-of-sample performance on held-out data. If you choose to tune the penalization parameter by cross validation, ensure that this is done only using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select only documents which appear after 1945\n",
    "X = dt_matrix.iloc[:][data_origin.year >= 1945]\n",
    "X = X.reset_index()\n",
    "X = X.drop('level_0', axis=1)\n",
    "\n",
    "# Create a new variable with only presidents and years\n",
    "parties = data_origin.loc[:,['president', 'year']]\n",
    "parties = parties.reset_index()\n",
    "\n",
    "# Create a new variable with 1 when presidents are Democrats, 0 when they are Republicans\n",
    "zero_len = pd.Series(np.zeros(len(parties.index)))\n",
    "parties['parties'] = zero_len\n",
    "parties['parties'] = (parties.president == \"Truman\") | (parties.president == \"Kennedy\") | (parties.president == \"Johnson\") | (parties.president == \"Carter\") | (parties.president == \"Clinton\") | (parties.president == \"Obama\")\n",
    "parties.parties = list(map(lambda x: 1 if x else 0, parties.parties))\n",
    "parties = parties.iloc[:][parties.year >= 1945]\n",
    "\n",
    "# Set the parties variable as y\n",
    "y = parties.parties\n",
    "\n",
    "# Split the X and y variables into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Create the logistic regression estimator with an l1 loss parameter\n",
    "log_reg = LogisticRegression(penalty = \"l1\")\n",
    "\n",
    "# Set some parameters to tune over\n",
    "c_space = [1.3, 1.5, 1.7]\n",
    "parameters = {'C': c_space}\n",
    "\n",
    "# Create a cross-validation estimator\n",
    "cv = GridSearchCV(log_reg, parameters)\n",
    "\n",
    "# Fit the model, predict and report the accuracy and the best parameter\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n",
    "\n",
    "## Run a logistic regression using the topics instead of the document-term matrix\n",
    "X = dt_matrix.iloc[:][data_origin.year >= 1945]\n",
    "X = ssp.csr_matrix(X.astype(int))\n",
    "\n",
    "K, S, alpha, eta = 20, 1000, 0.1, 0.01\n",
    "\n",
    "Col_Gibbs = lda.LDA(n_topics=K, n_iter=S, alpha=alpha, eta=eta)\n",
    "\n",
    "X = Col_Gibbs.fit_transform(X)\n",
    "\n",
    "# Split the X and y variables into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 59)\n",
    "\n",
    "# Create the logistic regression estimator\n",
    "log_reg = LogisticRegression(\"l2\")\n",
    "\n",
    "# Set some parameters to tune over\n",
    "c_space = [0.5, 2, 5]\n",
    "parameters = {'C': c_space}\n",
    "\n",
    "# Create a cross-validation estimator\n",
    "cv = GridSearchCV(log_reg, parameters)\n",
    "\n",
    "# Fit the model, predict and report the accuracy and the best parameter\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
