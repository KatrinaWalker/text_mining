{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "#### Implement the uncollapsed Gibbs sampler for latent Dirichlet allocation we discussed in class. Apply it to state-of-the-union addresses at a level of aggregation you choose and describe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "#### Find and run a version of the collapsed Gibbs sampler using lda in python. Run it for the same parameter values (i.e. Dirichlet hyperparameters and K) and documents you chose for the question above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Plot the perplexity across sampling iterations beginning from several diﬀerent starting values for each algorithm. Which algorithm appears to burn in faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) For the best-ﬁt chain from each algorithm after the burn-in period, construct estimates of the predictive distribution of θd for each document across a number of draws. Recall that the kth element of the predictive distribution is the number of times topic k is observed in document d plus α over Kα + Nd. Are the average values of these predictive distributions similar in the uncollapsed and collapsed samplers? How variable are these predictive distributions in the two algorithms across sample draws?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "#### Compare the classiﬁcation performance of a penalized logistic regression (which is implemented in scikit learn) when paragraphs are represented as unigram counts over raw terms versus topic shares. Use training samples to estimate the relationship between document content and political party, and then assess its out-of-sample performance on held-out data. If you choose to tune the penalization parameter by cross validation, ensure that this is done only using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
